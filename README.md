# ğŸ¤– Chatbot Intelligent E-commerce - SystÃ¨me RAG

Chatbot intelligent basÃ© sur RAG (Retrieval Augmented Generation) pour rÃ©pondre aux questions FAQ e-commerce.

## ğŸ“‹ Table des matiÃ¨res

- [Architecture](#architecture)
- [PrÃ©requis](#prÃ©requis)
- [Installation](#installation)
- [Utilisation](#utilisation)
- [API Endpoints](#api-endpoints)
- [Docker](#docker)
- [Structure du projet](#structure-du-projet)
- [DÃ©monstration](#dÃ©monstration)

## ğŸ—ï¸ Architecture

Le projet est composÃ© de plusieurs modules :
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Utilisateurâ”‚
â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”˜
â”‚
â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ FastAPI â”‚ (main.py)
â”‚ Endpoints â”‚
â””â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”˜
â”‚ â”‚
â–¼ â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ RAG â”‚ â”‚ LLM â”‚
â”‚ Module â”‚ â”‚Generator â”‚
â”‚(helpers)â”‚ â”‚ (llm.py) â”‚
â””â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”˜ â””â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”˜
â”‚ â”‚
â–¼ â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ChromaDB â”‚ â”‚ Gemini/ â”‚
â”‚(Vector DB)â”‚ â”‚ Fallback â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜


### Composants :

1. **FastAPI** : API REST exposant les endpoints
2. **RAG Module** : Gestion de ChromaDB et recherche vectorielle (`helpers/chromadb.py`)
3. **LLM Generator** : GÃ©nÃ©ration de rÃ©ponses avec Gemini ou mode fallback (`llm.py`)
4. **ChromaDB** : Base de donnÃ©es vectorielle pour la recherche sÃ©mantique

## ğŸ“¦ PrÃ©requis

- Python 3.11+
- Docker & Docker Compose (optionnel)
- ClÃ© API Google Gemini (optionnel) : [Obtenir une clÃ©](https://makersuite.google.com/app/apikey)

## ğŸš€ Installation

### Option 1 : Installation locale (RecommandÃ©e pour test rapide)

#### 1. TÃ©lÃ©charger le projet

TÃ©lÃ©chargez et extrayez le projet dans un dossier.

#### 2. CrÃ©er un environnement virtuel


# Windows
python -m venv venv
venv\Scripts\activate

# Linux/Mac
python3 -m venv venv
source venv/bin/activate
3. Installer les dÃ©pendances
bash
pip install -r requirements.txt
4. TÃ©lÃ©charger le dataset
TÃ©lÃ©chargez le dataset depuis Kaggle

Placez le fichier ecommerce_faq_dataset.csv dans le dossier data/

5. Initialiser la base de donnÃ©es

python -m helpers.init_data

6. Lancer l'API

uvicorn main:app --reload
L'API sera accessible sur : http://localhost:8000

Option 2 : Installation avec Docker
1. TÃ©lÃ©charger le dataset
Placez ecommerce_faq_dataset.csv dans le dossier data/

2. Construire et lancer les conteneurs

docker-compose up --build

3. Initialiser les donnÃ©es (premiÃ¨re fois uniquement)

docker-compose exec fastapi-app python -m helpers.init_data
L'API sera accessible sur : http://localhost:8000

ğŸ“– Utilisation
AccÃ©der Ã  la documentation Swagger
Ouvrez votre navigateur : http://localhost:8000/docs

Tester l'API avec curl

1. Route racine

curl http://localhost:8000/

2. Recherche de documents

curl -X POST http://localhost:8000/search \
  -H "Content-Type: application/json" \
  -d '{"query": "What payment methods do you accept?"}'

3. Chat complet

curl -X POST http://localhost:8000/chat \
  -H "Content-Type: application/json" \
  -d '{"query": "How can I track my order?"}'
ğŸ”Œ API Endpoints
GET /
Informations sur l'API

RÃ©ponse :


{
  "version": "1.0.0",
  "swagger": "/docs"
}
POST /search
Recherche les 5 documents les plus pertinents

RequÃªte :


{
  "query": "What payment methods do you accept?",
  "n_results": 5
}
RÃ©ponse :


{
  "query": "What payment methods do you accept?",
  "n_results": 5,
  "documents": [
    {
      "document": "Question: ... Reponse: ...",
      "question": "...",
      "answer": "...",
      "distance": 0.25
    }
  ]
}
POST /chat
Endpoint principal : recherche + gÃ©nÃ©ration de rÃ©ponse

RequÃªte :


{
  "query": "How can I return a product?"
}
RÃ©ponse :


{
  "query": "How can I return a product?",
  "top_documents": [...],
  "response": "Pour retourner un produit..."
}
GET /stats
Statistiques de la base de donnÃ©es

RÃ©ponse :


{
  "total_documents": 79,
  "collection_name": "ecommerce_faq",
  "status": "active"
}
GET /health
VÃ©rification de santÃ© de l'API

RÃ©ponse :


{
  "status": "healthy",
  "service": "chatbot-rag-api"
}
ğŸ³ Docker
Commandes utiles

# Lancer les services
docker-compose up -d

# Voir les logs
docker-compose logs -f

# ArrÃªter les services
docker-compose down

# Reconstruire l'image
docker-compose up --build

# AccÃ©der au conteneur
docker-compose exec fastapi-app bash

# Initialiser les donnÃ©es
docker-compose exec fastapi-app python -m helpers.init_data
Volumes persistants
Les donnÃ©es ChromaDB sont persistÃ©es dans le dossier ./data/chroma_langchain_db grÃ¢ce aux volumes Docker.

ğŸ“ Structure du projet

chatbot_rag_project/
â”‚
â”œâ”€â”€ ğŸ“‚ data/                           # DonnÃ©es
â”‚   â”œâ”€â”€ chroma_langchain_db/          # Base ChromaDB persistÃ©e
â”‚   â””â”€â”€ ecommerce_faq_dataset.csv     # Dataset (Ã  tÃ©lÃ©charger)
â”‚
â”œâ”€â”€ ğŸ“‚ helpers/                        # Modules utilitaires
â”‚   â”œâ”€â”€ chromadb.py                   # Module RAG (ChromaDB)
â”‚   â””â”€â”€ init_data.py                  # Script d'initialisation
â”‚
â”œâ”€â”€ main.py                           # API FastAPI principale
â”œâ”€â”€ llm.py                            # GÃ©nÃ©rateur LLM (Gemini)
â”œâ”€â”€ requirements.txt                  # DÃ©pendances Python
â”œâ”€â”€ Dockerfile                        # Configuration Docker
â”œâ”€â”€ docker-compose.yml                # Orchestration Docker
â”œâ”€â”€ .env.example                      # Template variables d'environnement
â”œâ”€â”€ .dockerignore                     # Fichiers ignorÃ©s par Docker
â”œâ”€â”€ README.md                         # Documentation principale
â”œâ”€â”€ DEMONSTRATION.md                  # Guide de dÃ©monstration
â”œâ”€â”€ QUICKSTART.md                     # Guide dÃ©marrage rapide
â”œâ”€â”€ test_api.py                       # Tests automatisÃ©s
â””â”€â”€ setup.py                          # Script de setup (optionnel)
ğŸ¬ DÃ©monstration
Pour une dÃ©monstration dÃ©taillÃ©e pas Ã  pas, consultez le fichier DEMONSTRATION.md.

Test rapide en 2 minutes :

# 1. Installer
pip install -r requirements.txt

# 2. Initialiser (rÃ©pondre 'o' si demandÃ©)
python -m helpers.init_data

# 3. Lancer
uvicorn main:app --reload

# 4. Tester dans le navigateur
# http://localhost:8000/docs
ğŸ”§ Configuration
Utiliser Google Gemini (optionnel)
Obtenez une clÃ© API sur Google AI Studio

CrÃ©ez un fichier .env Ã  partir du template :


cp .env.example .env
Ã‰ditez .env et ajoutez votre clÃ© :

env
GOOGLE_API_KEY=votre_cle_api_ici
Mode Fallback
Si Gemini n'est pas configurÃ©, le systÃ¨me utilise automatiquement un mode fallback qui retourne la rÃ©ponse du document le plus pertinent.

ğŸ¯ FonctionnalitÃ©s
âœ… Chargement et indexation de 79 FAQ e-commerce
âœ… Recherche vectorielle avec ChromaDB
âœ… GÃ©nÃ©ration de rÃ©ponses avec LLM (Gemini + Fallback)
âœ… API REST avec FastAPI
âœ… Documentation Swagger automatique
âœ… Dockerisation complÃ¨te
âœ… Persistance des donnÃ©es
âœ… Tests automatisÃ©s
âœ… Documentation exhaustive

ğŸ“ Notes techniques
Le systÃ¨me utilise ChromaDB en mode PersistentClient pour la persistance

Les embeddings sont gÃ©nÃ©rÃ©s automatiquement par ChromaDB (all-MiniLM-L6-v2)

La base de donnÃ©es est stockÃ©e dans data/chroma_langchain_db/

L'API suit les standards REST avec validation Pydantic

ğŸ§ª Tests
ExÃ©cutez les tests automatisÃ©s :


python test_api.py
Ou testez manuellement via Swagger UI : http://localhost:8000/docs

ğŸ¤ Contribution
Ce projet a Ã©tÃ© rÃ©alisÃ© dans le cadre d'un examen. Pour toute question, contactez l'auteur.

ğŸ“„ Licence
MIT License

Auteur : Takoi RIZGUI
Date : 04-02-2026
Examen : Chatbot Intelligent - SystÃ¨me RAG

Pour une dÃ©monstration complÃ¨te, exÃ©cutez python -m helpers.init_data puis uvicorn main:app --reload et accÃ©dez Ã  http://localhost:8000/docs



## ğŸ“ **Changements apportÃ©s :**

1. **Mise Ã  jour de l'architecture** : Montre la nouvelle structure avec `helpers/`
2. **Correction des chemins** : `data/chroma_langchain_db` au lieu de `chroma_db`
3. **Mise Ã  jour des commandes** : `python -m helpers.init_data` au lieu de `python init_data.py`
4. **Ajout de la section DÃ©monstration** : Lien vers `DEMONSTRATION.md`
5. **Structure du projet** : Montre la nouvelle organisation
6. **Notes techniques** : Informations sur ChromaDB et embeddings
7. **Instructions clarifiÃ©es** : Pour l'enseignant qui teste

## ğŸ¯ **Pour l'enseignant :**

Avec ce `README.md` mis Ã  jour, l'enseignant peut :
1. Comprendre la nouvelle structure
2. Suivre les bonnes commandes
3. Tester rapidement avec la section "DÃ©monstration"
4. AccÃ©der Ã  toutes les informations nÃ©cessaires